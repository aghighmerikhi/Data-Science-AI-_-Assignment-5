{"cells":[{"cell_type":"markdown","metadata":{"id":"R3D82waLqItO","cell_id":"ee4c1939845342739bd746bdffc4adcf","deepnote_cell_type":"markdown"},"source":"# DAT565 Introduction to Data Science and AI \n## 2023-2024, LP1\n## Assignment 5: Reinforcement Learning and Classification\n## Module 5 Group 32: Aghigh Merikhi(15 h) - Seyedehnaghmeh Mosaddeghi(15 h)\n\nThe exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n\nThe exercise takes place in this notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n\n*Tips:* \n* You can execute certain Linux shell commands by prefixing the command with a `!`. \n* You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results, the second you can use to write code snippets that execute the tasks required.  \n\nThis assignment is about **sequential decision making** under uncertainty (reinforcement learning). In a sequential decision process, the process jumps between different states (the *environment*), and in each state the decision maker, or *agent*, chooses among a set of actions. Given the state and the chosen action, the process jumps to a new state. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal *policy*) that maximizes the accumulated rewards.\n\nWe will use **Markov decision processes** (MDPs) to model the environment, and below is a primer on the relevant background theory. \n","block_group":"148c11809cb04e379ee3c3363ed3e0ab"},{"cell_type":"markdown","metadata":{"id":"8jEcC9NKqItQ","cell_id":"a8687024425642579e832c857ac00adf","deepnote_cell_type":"markdown"},"source":"\n* To make things concrete, we will first focus on decision making under **no** uncertainity (questions 1 and 2), i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n\n* (optional) Next we will work through one type of reinforcement learning algorithm called Q-learning (question 3). Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.\n\n* Finally, in question 4 you will be asked to explain differences between reinforcement learning and supervised learning and in question 5 write about decision trees and random forests.","block_group":"2b234e2090f746ae8af9ec6ac9dd2a1b"},{"cell_type":"markdown","metadata":{"id":"uGtknnUVqItP","cell_id":"2e39e415683d44f6ad5dd328b0f09c31","deepnote_cell_type":"markdown"},"source":"## Primer\n### Decision Making\nThe problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\ntwo parts. First, how do we learn about the world? This involves both the\nproblem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\ncurrently know about the world, how should we decide what to do, taking into\naccount future events and observations that may change our conclusions?\nTypically, this will involve creating long-term plans covering possible future\neventualities. That is, when planning under uncertainty, we also need to take\ninto account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\nthings should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\nknown to produce good results and experiment with something new is known\nas the **exploration-exploitation dilemma**.\n\n### The exploration-exploitation trade-off\n\nConsider the problem of selecting a restaurant to go to during a vacation. Lets say the\nbest restaurant you have found so far was **Les Epinards**. The food there is\nusually to your taste and satisfactory. However, a well-known recommendations\nwebsite suggests that **King’s Arm** is really good! It is tempting to try it out. But\nthere is a risk involved. It may turn out to be much worse than **Les Epinards**,\nin which case you will regret going there. On the other hand, it could also be\nmuch better. What should you do?\nIt all depends on how much information you have about either restaurant,\nand how many more days you’ll stay in town. If this is your last day, then it’s\nprobably a better idea to go to **Les Epinards**, unless you are expecting **King’s\nArm** to be significantly better. However, if you are going to stay there longer,\ntrying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\nbetter food for the remaining time, while otherwise you will have missed only\none good meal out of many, making the potential risk quite small.","block_group":"ba07e7c4a7664edfb139cff26768fe8b"},{"cell_type":"markdown","metadata":{"id":"h9WIePUCqItR","cell_id":"565c1dc218ef4a33b85556c075e73e9b","deepnote_cell_type":"markdown"},"source":"### Markov Decision Processes\nMarkov Decision Processes (MDPs) provide a mathematical framework for modeling sequential decision making under uncertainty. An *agent* moves between *states* in a *state space* choosing *actions* that affects the transition probabilities between states, and the subsequent *rewards* recieved after a jump. This is then repeated a finite or infinite number of epochs. The objective, or the *solution* of the MDP, is to optimize the accumulated rewards of the process.\n\nThus, an MDP consists of five parts: \n\n* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n* Transition probabilities $p(s_{t+1}|s_t,a_t)$ for jumping from state $s_t$ to state $s_{t+1}$ after taking action $a_t$\n* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ resulting from the chosen action and subsequent transition\n\nA *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions for each state. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n\n$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n\nwhere $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if we think all future rewards should count equally, we would use $\\gamma = 1$, while if we value near-future rewards higher than more distant rewards, we would use $\\gamma < 1$. The expected total *discounted* reward then becomes\n\n$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n\nNow, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is, we want to find the policy where\n\n$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n\nTo solve this we use a dynamic programming equation called the *Bellman equation*, given by\n\n$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n\nIt can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n\nA real world example would be an inventory control system. The states could be the amount of items we have in stock, and the actions would be the amount of items to order at the end of each month. The discrete time would be each month and the reward would be the profit. \n","block_group":"19b5ec22b2804e35b65e712a6cfb3ce9"},{"cell_type":"markdown","metadata":{"id":"KiO_zpY7qItS","cell_id":"98eeffbd6a864f4e87c8da4ceb9798eb","deepnote_cell_type":"markdown"},"source":"## Question 1","block_group":"9f5ebfbb603a4ece8fd2417d55cd2b89"},{"cell_type":"markdown","metadata":{"id":"XUyGq4olqItS","cell_id":"1dffb3c51ca44752aaad3e6ffea52f3b","deepnote_cell_type":"markdown"},"source":"The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n\n* The agent starts in state **S** (see table below)\n* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n* When reaching **F**, the game ends (absorbing state).\n* The numbers in the boxes represent the rewards you receive when moving into that box. \n* Assume no discount in this model: $\\gamma = 1$\n    \n    \n| | | |\n|----------|----------|---------|\n|-1 |1|**F**|\n|0|-1|1|  \n|-1 |0|-1|  \n|**S**|-1|1|\n\nLet $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n","block_group":"8ff5ede7489d41b283148d74c85968da"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":148,"fromCodePoint":0}],"cell_id":"22b6c41f07ae4db5afbf6ae6f26b87f7","deepnote_cell_type":"text-cell-p"},"source":"1a)What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. For instance, NESW will make a circle. ","block_group":"dceae8ba13d04777a0abffb5daa451d1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1ecc2dbbcb6c4dd1867ab964044c8b87","deepnote_cell_type":"text-cell-p"},"source":"The rewards associated with transitioning between states are as follows:\n - Moving  (N) or  (S) results in a reward of -1.\n - Moving (E) results in a reward of 1.\n - Moving (W) results in a reward of 0.\nThe goal is to find the optimal path that leads from 'S' to 'F' while maximizing the cumulative reward. One possible optimal path is 'EENNN'. The cumulative reward of -1 + 1+(-1) + 1 = 0 indicates that this path is optimal in terms of maximizing the total reward.  Paths like EENNWNE and is valid and result in a score of 0. Thus, the optimal path in the MDP can have multiple valid solutions with the same score.","block_group":"cc1d262a71814e8cb3c097101e8dd7d2"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":129,"fromCodePoint":0}],"cell_id":"0f37b56f297341f383fae1fc12cc6286","deepnote_cell_type":"text-cell-p"},"source":"1b)What is the optimal policy (i.e., the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid.","block_group":"a9484f2bc3bc4cf7a61885c4dc4a9a1e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cb8e6b030e59409582d4ef7d0e9a0ae8","deepnote_cell_type":"text-cell-p"},"source":"Position Optimal Action Reward for Action","block_group":"2d8baccb427d40aa9059107e9b4fd1c1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"60b1d055ca124dcdaf57b4cb4d45aa5a","deepnote_cell_type":"text-cell-p"},"source":"Position    optimal action   optimal reward\r\n(0,0) =>           N/E           =>      -1 \r\n(0,1)  =>           E/N           =>      0\r\n(0,2) =>          N/E/S         =>     -1 \r\n(0,3)  =>             E            =>      1\r\r\n(1,0)  =>              E            =>      1\r\n(1,1)   =>        N/E/W/S      =>    -1\r\n(1,2)  =>           N/E            =>     1 \r\n(1,3) =>             E               =>     F\r\n(2,0) =>          N/W            =>    -1\r\n(2,1)  =>           N/S            =>     1\r\n(2,2) =>            N               =>     F\r\n\r\n","block_group":"cda048845c2f46b2bdc109ff0558ba77"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":54,"fromCodePoint":0}],"cell_id":"80dc98ebbc3046628957767835b83105","deepnote_cell_type":"text-cell-p"},"source":"1c)What is expected total reward for the policy in 1a?","block_group":"af7e06a952a941f3bfad42ae02c90a3a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"debcbf6bdb6a47c09fcb6212a5cfb6e9","deepnote_cell_type":"text-cell-p"},"source":"As explained above, the expected total reward for the optimal policy 'EENNN' is 0. This indicates that the agent successfully navigates from 'S' to 'F' while maximizing its cumulative reward. This path results in the agent navigating through the MDP and accumulating rewards as it moves, ultimately reaching the goal state 'F' with a cumulative reward of 0. Furthermore, other paths can also achieve a total reward of 0 in this MDP, making the optimal solution not unique. The MDP allows for multiple valid paths to reach the same maximum achievable reward.","block_group":"4887e5defc1a448fbe2df7687549ae97"},{"cell_type":"markdown","metadata":{"id":"sNkIk-k7qItT","cell_id":"4635f74aaa954fd9a999482d5dcde690","deepnote_cell_type":"markdown"},"source":"## Value Iteration","block_group":"2a70b06a38f44b2ab4756131fa6b7fc2"},{"cell_type":"markdown","metadata":{"id":"NJTFDikEqItT","cell_id":"7a3f51e0ea1e426b9e4a2e2c5b297095","deepnote_cell_type":"markdown"},"source":"For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    ","block_group":"0ec34e8fd3dc4e38b305f207629717d4"},{"cell_type":"markdown","metadata":{"id":"3ZdhW0AZDoZv","cell_id":"bc820b82874740e792f7e79b9dbcc1f0","deepnote_cell_type":"markdown"},"source":"The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n\n```\nepsilon is a small value, threshold\nfor x from i to infinity \ndo\n    for each state s\n    do\n        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n    end\n    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n        for each state s,\n        do\n            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            return π, V_k \n        end\nend\n\n```","block_group":"4f0632020837459d94421739b7ef80e9"},{"cell_type":"markdown","metadata":{"id":"Nz3UqgozqItU","cell_id":"42ca32083741445aaf77aff21feae705","deepnote_cell_type":"markdown"},"source":"**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probability 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n\n**Reward**:\n\n| | | |  \n|----------|----------|---------|  \n|0|0|0|\n|0|10|0|  \n|0|0|0|  \n\n\n**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n\n| | | |  \n|----------|----------|---------|  \n|0|8|0|\n|8|2|8|  \n|0|8|0|  \n  \n**Iteration 2**:  \n  \nStaring with cell (0,0) (lower left corner): We find the expected value of each move:  \nAction **S**: 0  \nAction **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **W**: 0\n\nHence any action between **E** and **N** would be best at this stage.\n\nSimilarly for cell (1,0):\n\nAction **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n\nSimilar calculations for remaining cells give us:\n\n| | | |  \n|----------|----------|---------|  \n|5.76|10.88|5.76|\n|10.88|8.12|10.88|  \n|5.76|10.88|5.76|  \n","block_group":"7eece24f9c85448987c003d5790df44f"},{"cell_type":"markdown","metadata":{"id":"S3vIdFpuqItU","cell_id":"1dba953738914457ba37c8f42f19fcfc","deepnote_cell_type":"markdown"},"source":"## Question 2\n\n**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n\n","block_group":"7db9c5ed52224010888a0fe5f2e53de8"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1696012606650,"execution_millis":117,"deepnote_to_be_reexecuted":false,"cell_id":"1c25ac955fc2499baeaee577f95fe03f","deepnote_cell_type":"code"},"source":"import numpy as np\n\n# Define reward and value matrices\nreward = np.array([[0, 0, 0],\n                    [0, 10, 0],\n                    [0, 0, 0]])\nvalue = np.zeros((3, 3))\n\n# Define constants\naction_probability = 0.8\ndiscount_factor = 0.9\nepsilon = 1e-4\n\nwhile True:\n    delta = 0\n    for x in range(3):\n        for y in range(3):\n            old_v = value[x, y]\n            actions = [('N', (x - 1, y)), ('S', (x + 1, y)), ('E', (x, y + 1)), ('W', (x, y - 1))]\n            expected_values = []\n\n            for a, s in actions:\n                if 0 <= s[0] <= 2 and 0 <= s[1] <= 2:\n                    expected_value = action_probability * (reward[s] + discount_factor * value[s])+ (1 - action_probability) * (reward[x, y] + discount_factor * value[x, y])\n                    expected_values.append(expected_value)\n                else:\n                    expected_value = action_probability * (reward[x, y] + discount_factor * value[x, y])\n                    expected_values.append(expected_value)\n\n            value[x, y] = max(expected_values)\n            delta = max(delta, abs(old_v - value[x, y]))\n\n    if delta < epsilon:\n        break\n\n# Print the value function at convergence\nprint('Converging optimal value function: ')\nprint()\nprint(np.round(value, 2))\n\n","block_group":"2219ead5f7ec4a9d85dc30b3018ef5ad","execution_count":null,"outputs":[{"name":"stdout","text":"Converging optimal value function: \n\n[[45.61 51.95 45.61]\n [51.95 48.05 51.95]\n [45.61 51.95 45.61]]\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":74,"fromCodePoint":0}],"cell_id":"e567cfeb265342a986044df87226e3d1","deepnote_cell_type":"text-cell-p"},"source":"2b) Explain why the result of 2a) does not depend on the initial value V0.","block_group":"817da20c02d84a1398cde782bcbf1fa7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"bf17836f15704b82b30088758f217b97","deepnote_cell_type":"text-cell-p"},"source":"In value iteration, the convergence criterion is based on the maximum change in the value function (that is, when the change goes below a specific threshold, represented as epsilon). This criterion ensures that the algorithm ends when the values have stabilized, resulting in the optimal solution. The convergence qualities of the method ensure that the result is independent of the initial values. While the initial values may affect the amount of iterations needed to converge, the ultimate solution is always the same—the optimal policy and value function for the particular MDP.","block_group":"54086a81e73342ca81e016a2a43c898f"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":234,"fromCodePoint":0}],"cell_id":"5680d481b0c2476f994c3fb42b7ce8bd","deepnote_cell_type":"text-cell-p"},"source":"2c)Describe your interpretation of the discount factor gamma. What would happen in the two extreme cases gamma = 0 and gamma = 1? Given some MDP, what would be important things to consider when deciding on which value of gamma to use?","block_group":"853eb44969724b4aa314f7a9f3d3e960"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0144bd6e35a9488884820e1d91fc6bb1","deepnote_cell_type":"text-cell-p"},"source":"The discount factor gamma determines the trade-off between immediate and future rewards. A higher gamma values indicate that the agent values long-term rewards more and is willing to delay immediate reward, whereas a lower gamma values indicate a preference for immediate rewards. Below are the two extreme cases:","block_group":"ebde816444bb4438bba45fcd2fdefbad"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8b0b0718242b4295a9157d878ae996c2","deepnote_cell_type":"text-cell-p"},"source":"γ = 0: The agent only considers and focuses on maximizing immediate rewards, and it ignores future rewards nd long-term effect completely. In many real-world settings, the agent's purpose is to maximize immediate rewards, which may not be the optimum strategy.","block_group":"22263c9e4d0a42c0aa216446e68ac0f1"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4b60401b1a31492bb0c15c65eaabae25","deepnote_cell_type":"text-cell-p"},"source":"γ = 1: The agent has a long-term perspective and considers future rewards as highly as immediate rewards in this extreme scenario. The agent takes into account the cumulative expected rewards over time and tries to maximize the overall expected reward throughout the entire Track. The agent is willing to postpone benefits in order to get larger rewards later. ","block_group":"0c8a0dbafb624db09617c5cfd720cfb8"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c4441a8a31094dd0aacc76ff76fb6e3f","deepnote_cell_type":"text-cell-p"},"source":"Choosing the appropriate discount factor value in a MDP is a vital decision that must balance short-term and long-term goals. Risk tolerance, time horizon, uncertainty, computing efficiency, and the discounting of distant rewards should all be variables in the selection. Experimentation and sensitivity analysis are frequently used to determine the ideal value that matches with the unique problem's dynamics and the agent's aims, while knowing that it fluctuates depending on the characteristics and goals of the situation.","block_group":"bd7163b020964a24887f5bb728b5e176"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":10,"fromCodePoint":0}],"cell_id":"63238867ce5448c0acfc4374141a01ed","deepnote_cell_type":"text-cell-p"},"source":"Question 4","block_group":"9345f4931b16477a8c2415834e2ce9f9"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":3,"fromCodePoint":0},{"type":"marks","marks":{"bold":true},"toCodePoint":92,"fromCodePoint":4}],"cell_id":"05508e18d5334214a4a27e518437bc91","deepnote_cell_type":"text-cell-p"},"source":"4a) What is the importance of exploration in reinforcement learning? Explain with an example.","block_group":"f77cab540cea4d2a83ec6a5a2b12d889"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4222fee1268c4bceaa275afaf42759fe","deepnote_cell_type":"text-cell-p"},"source":"Exploration is an important concept in reinforcement learning (RL) because it enables an agent to learn about its environment and discover optimal strategies. In RL, an agent often starts with limited knowledge of the environment and must explore different actions and states to gather information. Without exploration, the agent might miss valuable rewards, get stuck in repeated actions, or never discover hidden opportunities. Balancing exploration and exploitation is essential for agents to learn and make optimal decisions in uncertain environments. For example, an online recommendation system that suggests movies to users. The system begins with no prior knowledge of users' preferences and must learn to make accurate movie recommendations. Exploration in this context involves recommending a variety of movies to different users, even if the system is uncertain about their preferences. And exploration allows the recommendation system to learn about users' movie preferences over time. \r\n","block_group":"f4b32f71c67045c68dbca17efd2613ec"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":3,"fromCodePoint":0},{"type":"marks","marks":{"bold":true},"toCodePoint":124,"fromCodePoint":4}],"cell_id":"82e4d11881254ed784090185d457a07f","deepnote_cell_type":"text-cell-p"},"source":"4b) Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.","block_group":"b53462522de8459783775fc7539e486c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d05c8b5213ea4884958980f241e7e17a","deepnote_cell_type":"text-cell-p"},"source":"In RL, agents tries to make decisions over time to maximize a cumulative reward, with feedback provided in the form of rewards based on their actions.  In contrast, supervised learning involves training a model to make predictions or classifications based on labeled data, without interactions with an environment. RL operates in environments where  labeled datasets are not available, it learns from exploration and trial and error, whereas supervised learning relies on available labeled data for training. RL deals with feedback delays, as rewards may be delayed in time and attributed to a sequence of actions, whereas supervised learning typically receives immediate and direct feedback for each prediction. ","block_group":"588eaee4ccfa4ebbb01b517181817941"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":10,"fromCodePoint":0}],"cell_id":"d97c2506e8d849839d74ff1e2a763031","deepnote_cell_type":"text-cell-p"},"source":"Question 5","block_group":"8d9f4a42e7df446cba2457a7a9add77b"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":85,"fromCodePoint":0}],"cell_id":"7a520447c95e418ba5fe6ef8a0b3d5ea","deepnote_cell_type":"text-cell-p"},"source":"5a) Give a summary of how a decision tree works and how it extends to random forests.","block_group":"664497634df84bf6bca9d98fe4226eea"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"06c2b50e53b34455a3039340817ffbfa","deepnote_cell_type":"text-cell-p"},"source":"A decision tree is a supervised machine learning approach that divides a dataset into subsets recursively according to feature values. Each decision node represents a characteristic and a splitting criterion, forming the decision nodes and leaf nodes of a tree-like structure. The method chooses the optimum features and split points to optimize a certain criteria, such as mean squared error reduction for regression or Gini impurity for classification. Data points follow the splits in the tree based on their feature values as they move from the root to a leaf node to create predictions. The final output is the prediction made at the leaf node.","block_group":"8df381990c5946c799dabe6f9e343f14"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"38dc6700fe2d4e73a62c14cbc5c3e005","deepnote_cell_type":"text-cell-p"},"source":"By assembling several different decision trees, Random Forests expand the idea of decision trees. Each tree examines just a random subset of characteristics at each node split and is trained on a random subset of the training data with replacement. In Random Forests, the final prediction is obtained by combining the predictions of individual trees, either by majority voting for classification problems or averaging for regression tasks. ","block_group":"24af9d87008a48b5944cfb9379d433a3"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":96,"fromCodePoint":0}],"cell_id":"7ba1694b12ee4bfdbfada5088a99e59f","deepnote_cell_type":"text-cell-p"},"source":"5b) State at least one advantage and one drawback with using random forests over decision trees.","block_group":"12915ec958884d9b887c9e3a211d1e63"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cc7eee24a2154ef08451f23d7c903881","deepnote_cell_type":"text-cell-p"},"source":"Random Forests have the following advantage over Decision Trees:","block_group":"48c0dfda91c34765bee686c3f08c4958"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ba16ea71192a46f183af4ada7d744d65","deepnote_cell_type":"text-cell-p"},"source":"Reduced Overfitting: When compared to individual decision trees, Random Forests are less prone to overfitting. They develop a more generic model by averaging predictions from many trees and employing random feature selection, which generally results in higher performance on unknown data. As a result, Random Forests are more robust and suitable for a broader range of datasets.","block_group":"fe1ae9a66eb04e61a9b8914139a6e031"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"36b4afeec08e4dc884af25fe339620ec","deepnote_cell_type":"text-cell-p"},"source":"The disadvantage of Random Forests over Decision Trees is as follows:","block_group":"3cdf0ed30036465394bf0ce6d05e8170"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d2f6bdbf61084b7fb5359a71a832f00d","deepnote_cell_type":"text-cell-p"},"source":"Random Forests are fundamentally more complex than individual decision trees since they are made up of several trees. This intricacy can make interpreting the model's predictions and understanding the significance of specific variables more difficult. A single decision tree, on the other hand, is usually easier to depict and describe.","block_group":"79260945833449efb1f9480ef5e12bb9"},{"cell_type":"markdown","metadata":{"id":"-yHCotQGqItV","cell_id":"6dccb57866ae45c08a9f5dcd7385cc9a","deepnote_cell_type":"markdown"},"source":"\n# References\nPrimer/text based on the following references:\n* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf","block_group":"ac087513b2834bea819f6d469b70f9af"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=05d86ed7-77dc-4d57-baa3-23063c711d41' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"vscode":{"interpreter":{"hash":"123ec00200262563abc7db73a7df297e3839d21b30cef8aa24288688fdbde7de"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.9.5","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"d406dcdee77f4cddbdd4ba3bba2cc050","deepnote_persisted_session":{"createdAt":"2023-09-29T19:01:26.483Z"},"deepnote_execution_queue":[]}}